0x03. Log Parsing
Algorithm
Python


Log parsing is the process of extracting relevant information and structured data from log files. Log files contain records of events or activities generated by various systems, applications, or devices. They are commonly used for troubleshooting, monitoring, and analysis purposes.


A log parsing algorithm is a set of instructions or a code implementation that automates the process of parsing log files. It typically involves pattern matching, regular expressions, and text processing techniques to extract meaningful data from the log entries.


Keep in mind that the log parsing algorithm needs to be tailored to the specific log file format you're working with. The regular expression pattern used in the example above might not match the format of your log file exactly, so you may need to modify it accordingly.



Requirements
General
Allowed editors: vi, vim, emacs
All your files will be interpreted/compiled on Ubuntu 14.04 LTS using python3 (version 3.4.3)
All your files should end with a new line
The first line of all your files should be exactly #!/usr/bin/python3
A README.md file, at the root of the folder of the project, is mandatory
Your code should use the PEP 8 style (version 1.7.x)
All your files must be executable
The length of your files will be tested using wc



Example in this file 0-generator.py
#!/usr/bin/python3
import random
import sys
from time import sleep
import datetime

for i in range(10000):
    sleep(random.random())
    sys.stdout.write("{:d}.{:d}.{:d}.{:d} - [{}] \"GET /projects/260 HTTP/1.1\" {} {}\n".format(
        random.randint(1, 255), random.randint(1, 255), random.randint(1, 255), random.randint(1, 255),
        datetime.datetime.now(),
        random.choice([200, 301, 400, 401, 403, 404, 405, 500]),
        random.randint(1, 1024)
    ))
    sys.stdout.flush()


The above code generates simulated log entries in the Common Log Format (CLF). It continuously writes 10,000 log entries to the standard output (stdout).


#!/usr/bin/python3
This line is called a shebang and is used to specify the interpreter to be used when executing the script. In this case, it indicates that the script should be run using Python 3.


import random
import sys
from time import sleep
import datetime
These lines import the necessary modules for the code: random for generating random numbers, sys for writing to stdout, sleep for introducing delays, and datetime for obtaining the current timestamp.


for i in range(10000):
This for loop iterates 10,000 times, generating log entries in each iteration.


    sleep(random.random())
This line introduces a random delay using the sleep function from the time module. The random.random() function generates a random floating-point number between 0 and 1, causing the script to pause for a random amount of time before generating the next log entry.



    sys.stdout.write("{:d}.{:d}.{:d}.{:d} - [{}] \"GET /projects/260 HTTP/1.1\" {} {}\n".format(
        random.randint(1, 255), random.randint(1, 255), random.randint(1, 255), random.randint(1, 255),
        datetime.datetime.now(),
        random.choice([200, 301, 400, 401, 403, 404, 405, 500]),
        random.randint(1, 1024)
    ))

This line generates a log entry in the Common Log Format (CLF) and writes it to the stdout using sys.stdout.write(). The log entry consists of an IP address, timestamp, HTTP request, response code, and response size. The IP address is generated by concatenating four random numbers between 1 and 255 separated by periods. The timestamp is obtained using datetime.datetime.now(). The HTTP request is fixed as "GET /projects/260 HTTP/1.1". The response code is randomly chosen from a list of HTTP status codes. The response size is a random integer between 1 and 1024.



    sys.stdout.flush()
This line flushes the stdout buffer, ensuring that the log entry is immediately written rather than waiting for the buffer to fill up.

Overall, this code simulates log entries in the Common Log Format and continuously writes them to stdout, mimicking the behavior of a web server generating access logs.






Example in this file 0-stats.py
#!/usr/bin/python3
"""  A script that Reads stdin line by line and computes metrics
"""
import sys


if __name__ == "__main__":
    status = {"200": 0,
              "301": 0,
              "400": 0,
              "401": 0,
              "403": 0,
              "404": 0,
              "405": 0,
              "500": 0}
    count = 1
    file_size = 0

    def get_line(line):
        """ parse and grab data"""
        try:
            parsed_line = line.split()
            status_code = parsed_line[-2]
            if status_code in status.keys():
                status[status_code] += 1
            return int(parsed_line[-1])
        except Exception:
            return 0

    def print_stats():
        """print stats"""
        print("File size: {}".format(file_size))
        for key in sorted(status.keys()):
            if status[key]:
                print("{}: {}".format(key, status[key]))

    try:
        for line in sys.stdin:
            file_size += get_line(line)
            if count % 10 == 0:
                print_stats()
            count += 1
    except KeyboardInterrupt:
        print_stats()
        raise
    print_stats()
    

This Python script is designed to read data from the standard input (stdin) line by line and compute certain metrics based on the data


mporting Libraries:
The script imports the "sys" module, which provides access to some variables used or maintained by the interpreter and functions that interact strongly with the interpreter. It is commonly used to access command-line arguments and stdin (standard input).

Initializing Variables:
The script initializes several variables:

status: This is a dictionary that will hold the counts of different HTTP status codes as keys. The initial counts for all status codes are set to 0.
count: This variable starts at 1 and is used to keep track of the number of lines read so far.
file_size: This variable is used to keep track of the total file size calculated from the last column of each input line.

Defining Functions:
There are two functions defined in the script:

get_line(line): This function takes a line of input as a parameter and tries to parse it. It splits the line into different parts and extracts the HTTP status code, which is assumed to be the second-to-last element in the line. If the status code exists in the status dictionary, it increments the count for that status code and returns the last element (assumed to be the file size). If any error occurs during parsing, it returns 0.
print_stats(): This function prints the computed statistics. It displays the total file size and the counts of different HTTP status codes that have been encountered.


Main Execution:
The main part of the script is within the __main__ block. It reads data from the standard input (stdin) line by line using a loop. For each line, it increments the count variable. It also adds the file size obtained from the line to the file_size variable by calling the get_line() function. If the count variable is a multiple of 10, it calls the print_stats() function to print the current statistics.

Handling KeyboardInterrupt:
The script handles the KeyboardInterrupt exception (usually triggered by pressing Ctrl+C) by printing the statistics collected so far and then raising the exception again to terminate the script gracefully.

Printing Final Statistics:
After the loop is finished, the script prints the final statistics, including the total file size and the counts of different HTTP status codes encountered during the execution of the script.


n summary, this script processes input data line by line, assumes the last column of each line contains the file size, and extracts the HTTP status code from the second-to-last column. It then keeps track of various HTTP status codes encountered and the total file size processed. Every ten lines, it prints the statistics, and at the end, it prints the final statistics.


